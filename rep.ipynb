{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdcd4cba",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing Using Core Python\n",
    "\n",
    "In this step, we clean the insurance claim dataset using basic Python (no external libraries like pandas/numpy).\n",
    "\n",
    "### Key Steps:\n",
    "- Read the CSV line-by-line using built-in `open()` and `readlines()`.\n",
    "- Strip and split each row, and check for:\n",
    "  - Correct number of columns\n",
    "  - Presence of required fields\n",
    "  - Valid numeric data (amounts)\n",
    "- Normalize text data (city names, rejection remarks)\n",
    "- Return a cleaned dataset as a list of lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ee9fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_csv(file_path):\n",
    "    \"\"\"\n",
    "    Preprocess the CSV data using only core Python.\n",
    "    Tasks:\n",
    "    - Remove rows with missing/invalid data.\n",
    "    - Convert numeric fields.\n",
    "    - Standardize city names.\n",
    "    - Validate mandatory fields.\n",
    "    \"\"\"\n",
    "    cleaned_data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    header = lines[0].strip().split(',')\n",
    "    cleaned_data.append(header)\n",
    "\n",
    "    for line in lines[1:]:\n",
    "        row = line.strip().split(',')\n",
    "\n",
    "        if len(row) != len(header):\n",
    "            continue  # skip rows with column mismatch\n",
    "\n",
    "        claim_id, claim_date, customer_id, claim_amount, premium_collected, paid_amount, city, rejection_remarks = row\n",
    "\n",
    "        # Validate required fields\n",
    "        if not all([claim_id, claim_date, customer_id, claim_amount, premium_collected, paid_amount, city]):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            claim_amount = float(claim_amount)\n",
    "            premium_collected = float(premium_collected)\n",
    "            paid_amount = float(paid_amount)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        city = city.strip().title()  # normalize city name\n",
    "        rejection_remarks = rejection_remarks.strip() if rejection_remarks else \"\"\n",
    "\n",
    "        cleaned_data.append([\n",
    "            claim_id.strip(), claim_date.strip(), customer_id.strip(),\n",
    "            claim_amount, premium_collected, paid_amount, city, rejection_remarks\n",
    "        ])\n",
    "    \n",
    "    return cleaned_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e1a4d1",
   "metadata": {},
   "source": [
    "## Part 2: City Analysis for Shutdown Recommendation\n",
    "\n",
    "XYZ TECH wants to shut down operations in one of the four cities: **Pune, Kolkata, Ranchi, or Guwahati**.\n",
    "\n",
    "### Objective:\n",
    "Identify the city that performs worst in terms of:\n",
    "- Least number of claims\n",
    "- Least total premium collected\n",
    "- Highest rejection rate (used as tiebreaker)\n",
    "\n",
    "### Steps:\n",
    "- Loop through cleaned data and calculate:\n",
    "  - Total claims per city\n",
    "  - Total premium collected\n",
    "  - Number of rejected claims (i.e., `PAID_AMOUNT == 0`)\n",
    "- Print stats per city\n",
    "- Return the city that is least beneficial to continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc9faf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_city_for_shutdown(cleaned_data):\n",
    "    \"\"\"\n",
    "    Recommend a city for shutdown based on:\n",
    "    - Least number of claims\n",
    "    - Least total premium collected\n",
    "    - Highest rejection ratio\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "\n",
    "    city_stats = defaultdict(lambda: {'claims': 0, 'total_premium': 0.0, 'rejected': 0})\n",
    "\n",
    "    header = cleaned_data[0]\n",
    "    city_idx = header.index(\"CITY\")\n",
    "    premium_idx = header.index(\"PREMIUM_COLLECTED\")\n",
    "    paid_idx = header.index(\"PAID_AMOUNT\")\n",
    "\n",
    "    for row in cleaned_data[1:]:\n",
    "        city = row[city_idx]\n",
    "        premium = float(row[premium_idx])\n",
    "        paid = float(row[paid_idx])\n",
    "\n",
    "        city_stats[city]['claims'] += 1\n",
    "        city_stats[city]['total_premium'] += premium\n",
    "        if paid == 0.0:\n",
    "            city_stats[city]['rejected'] += 1\n",
    "\n",
    "    print(\"City-wise stats:\\n\")\n",
    "    for city, stats in city_stats.items():\n",
    "        rejection_rate = stats['rejected'] / stats['claims']\n",
    "        print(f\"{city}: Claims={stats['claims']}, Premium={stats['total_premium']}, Rejection Rate={rejection_rate:.2f}\")\n",
    "\n",
    "    # Heuristic: prioritize low claims, low premium, high rejection\n",
    "    city_to_consider = min(\n",
    "        city_stats.items(),\n",
    "        key=lambda item: (\n",
    "            item[1]['claims'],\n",
    "            item[1]['total_premium'],\n",
    "            -item[1]['rejected'] / item[1]['claims']\n",
    "        )\n",
    "    )[0]\n",
    "\n",
    "    return city_to_consider\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb1f294",
   "metadata": {},
   "source": [
    "## Part 3: Fix and Apply `complex_rejection_classifier`\n",
    "\n",
    "We are provided a function `complex_rejection_classifier` that classifies `REJECTION_REMARKS` into broader categories.\n",
    "\n",
    "### Categories:\n",
    "- `Policy Issue`\n",
    "- `Fraudulent Claim`\n",
    "- `Documentation Issue`\n",
    "- `Coverage Issue`\n",
    "- `Pre-existing Condition`\n",
    "- `Late Filing`\n",
    "- `Other`\n",
    "\n",
    "### Steps:\n",
    "- Fix any syntax or logic issues in the classifier\n",
    "- Normalize remarks (lowercase, strip)\n",
    "- Apply classification to each row\n",
    "- Add a new column `REJECTION_CLASS` to the cleaned dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "951b36a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_rejection_classifier(remark):\n",
    "    remark = remark.lower().strip()\n",
    "    if 'policy lapsed' in remark:\n",
    "        return 'Policy Issue'\n",
    "    # elif 'fraud' in remark or 'fake' in remark:\n",
    "    #     return 'Fraudulent Claim'\n",
    "    # elif 'document' in remark or 'missing' in remark:\n",
    "    #     return 'Documentation Issue'\n",
    "    # elif 'not covered' in remark or 'exclusion' in remark:\n",
    "    #     return 'Coverage Issue'\n",
    "    # elif 'pre-existing' in remark:\n",
    "    #     return 'Pre-existing Condition'\n",
    "    # elif 'late' in remark or 'delay' in remark:\n",
    "    #     return 'Late Filing'\n",
    "    elif \"fake_doc\" in remark:\n",
    "        return 'Fake_Document'\n",
    "    elif \"not_covered\" in remark:\n",
    "        return 'Not_Covered'\n",
    "    elif \"policy_expired\" in remark:\n",
    "        return 'Policy_expired'\n",
    "    else:\n",
    "        return 'Other'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f73c44",
   "metadata": {},
   "source": [
    "## Applying the `complex_rejection_classifier` Function\n",
    "\n",
    "Now that we have cleaned the data, we proceed to classify the reasons for rejected claims using a custom classification function.\n",
    "\n",
    "### Goal:\n",
    "Create a new column `REJECTION_CLASS` based on keywords in the `REJECTION_REMARKS` field.\n",
    "\n",
    "### Steps:\n",
    "1. Define the corrected `complex_rejection_classifier` function.\n",
    "2. For each row in the cleaned data:\n",
    "   - If `REJECTION_REMARKS` is non-empty and `PAID_AMOUNT == 0`, classify the reason.\n",
    "   - Otherwise, assign `'No Remark'` or `'Not Rejected'` accordingly.\n",
    "3. Append the new classification column to the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36618720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_rejection_remarks(cleaned_data):\n",
    "    header = cleaned_data[0] + ['REJECTION_CLASS']\n",
    "    classified_data = [header]\n",
    "\n",
    "    remark_idx = cleaned_data[0].index(\"REJECTION_REMARKS\")\n",
    "\n",
    "    for row in cleaned_data[1:]:\n",
    "        remark = row[remark_idx]\n",
    "        if remark:\n",
    "            rejection_class = complex_rejection_classifier(remark)\n",
    "        else:\n",
    "            rejection_class = 'No Remark'\n",
    "        classified_data.append(row + [rejection_class])\n",
    "    \n",
    "    return classified_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4276435f",
   "metadata": {},
   "source": [
    "###  Output:\n",
    "Cleaned data with an additional column `REJECTION_CLASS`, ready for further analysis or export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85eecfbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City-wise stats:\n",
      "\n",
      "Pune: Claims=30, Premium=295452.03, Rejection Rate=0.10\n",
      "Guwahati: Claims=22, Premium=254727.02000000005, Rejection Rate=0.09\n",
      "Ranchi: Claims=13, Premium=114105.14, Rejection Rate=0.15\n",
      "Kolkata: Claims=11, Premium=108026.83999999998, Rejection Rate=0.00\n",
      "\n",
      " Recommended City for Shutdown: Kolkata\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Preprocess\n",
    "file_path = \"Insurance_auto_data.csv\"\n",
    "cleaned = preprocess_csv(file_path)\n",
    "\n",
    "# Step 2: City Recommendation\n",
    "recommended_city = analyze_city_for_shutdown(cleaned)\n",
    "print(f\"\\n Recommended City for Shutdown: {recommended_city}\")\n",
    "\n",
    "# Step 3: Classification\n",
    "final_data = classify_rejection_remarks(cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1087bdab",
   "metadata": {},
   "source": [
    "### Step 4: Save Cleaned and Classified Data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2135933a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned and classified data saved to: cleaned_insurance_data_with_classes.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Save Cleaned and Classified Data to CSV\n",
    "\n",
    "def save_to_csv(data, output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for row in data:\n",
    "            # Ensure all values are converted to strings and properly escaped\n",
    "            escaped_row = [str(value).replace(\",\", \" \") for value in row]\n",
    "            line = \",\".join(escaped_row)\n",
    "            f.write(line + \"\\n\")\n",
    "\n",
    "# Save to CSV file\n",
    "output_file = \"cleaned_insurance_data_with_classes.csv\"\n",
    "save_to_csv(final_data, output_file)\n",
    "\n",
    "print(f\"\\nCleaned and classified data saved to: {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
